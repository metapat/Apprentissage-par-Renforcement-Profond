{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metapat/Apprentissage-par-Renforcement-Profond/blob/main/notebooks/unit2/unit2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "## Unit 2: Q-Learning avec FrozenLake-v1 ‚õÑ et Taxi-v3 üöï\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
        "\n",
        "Dans ce notebook, **vous allez coder votre premier agent d'apprentissage par renforcement de A √† Z** pour jouer √† Frozen Lake ‚ùÑÔ∏è en utilisant le Q-Learning, le partager avec la communaut√© et exp√©rimenter diff√©rentes configurations. ‚¨áÔ∏è\n",
        "\n",
        "Voici un exemple de ce que **vous r√©aliserez en quelques minutes seulement.** ‚¨áÔ∏è\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRU_vXBrl1Jx"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üéÆ Environnements:\n",
        "\n",
        "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
        "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "\n",
        "###üìö RL-Library:\n",
        "\n",
        "- Python and NumPy\n",
        "- [Gymnasium](https://gymnasium.farama.org/)\n",
        "\n",
        "Nous nous effor√ßons constamment d'am√©liorer nos tutoriels, donc **si vous rencontrez des probl√®mes dans ce notebook**, veuillez [ouvrir un ticket sur le d√©p√¥t GitHub](https://github.com/huggingface/deep-rl-class/issues)."
      ],
      "metadata": {
        "id": "DPTBOv9HYLZ2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6tjI2tHQ8j"
      },
      "source": [
        "## Objectifs de ce notebook üèÜ\n",
        "√Ä la fin de ce notebook, vous serez capable de :\n",
        "- Utiliser **Gymnasium**, la biblioth√®que d'environnement.\n",
        "- Programmer un agent Q-Learning de A √† Z.\n",
        "- Soumettre votre agent entra√Æn√© et le code au Hub, accompagn√©s d'une vid√©o de d√©monstration et d'une note d'√©valuation üî•.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ce cahier provient du cours d'apprentissage par renforcement profond.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ],
      "metadata": {
        "id": "viNzVbVaYvY3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5HnEefISCB"
      },
      "source": [
        "Dans ce cours gratuit, vous allez :\n",
        "- üìñ √âtudier l‚Äôapprentissage par renforcement profond, tant sur le plan th√©orique que pratique.\n",
        "- üßë‚Äçüíª Apprendre √† utiliser des biblioth√®ques d‚Äôapprentissage par renforcement profond reconnues, telles que Stable Baselines3, RL Baselines3 Zoo, CleanRL et Sample Factory 2.0.\n",
        "- ü§ñ Entra√Æner des agents dans des environnements uniques. Pour en savoir plus, consultez le programme.\n",
        " üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "N'oubliez pas de vous inscrire au cours √† cette adresse : http://eepurl.com/ic5ZUD (nous recueillons votre adresse e-mail afin de vous envoyer les liens d√®s la publication de chaque module et de vous informer des d√©fis et des mises √† jour). Pour rester en contact, rejoignez notre serveur Discord et √©changez avec la communaut√© et avec nous üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mo_6rXIjRi"
      },
      "source": [
        "## Pre requis üèóÔ∏è\n",
        "\n",
        "Avant de vous plonger dans le cahier, vous devez : üî≤ üìö **√âtudier [Apprentissage par questions-r√©ponses en lisant l'unit√© 2](https://huggingface.co/deep-rl-course/unit2/introduction)**  ü§ó  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ONOODsyrMU"
      },
      "source": [
        "## Petit r√©capitulatif du Q-Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68VveLacfxJ"
      },
      "source": [
        "L'apprentissage par renforcement Q-Learning est un algorithme qui :\n",
        "- Entra√Æne une fonction Q, une fonction associant une valeur √† une action. Cette fonction est encod√©e en m√©moire interne par une table Q contenant toutes les paires √©tat-action.\n",
        "- √âtant donn√© un √©tat et une action, la fonction Q recherche la valeur correspondante dans la table Q.\n",
        "\n",
        "    \n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n",
        "\n",
        "Une fois l'entra√Ænement termin√©, nous obtenons une fonction Q optimale, et donc une table Q optimale.\n",
        "\n",
        "Et si nous disposons d'une fonction Q optimale, nous avons une politique optimale, car nous connaissons, pour chaque √©tat, la meilleure action √† entreprendre.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\n",
        "\n",
        "\n",
        "Au d√©but, notre **table Q est inutile car elle attribue une valeur arbitraire √† chaque paire √©tat-action (la plupart du temps, nous l'initialisons √† 0)**. Mais √† mesure que nous explorerons l'environnement et mettrons √† jour notre table Q, elle nous fournira des approximations de plus en plus pr√©cises.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
        "\n",
        "Voici le pseudocode du Q-Learning :\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programmons notre premier algorithme d'apprentissage par renforcement üöÄ"
      ],
      "metadata": {
        "id": "HEtx8Y8MqKfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour valider cette exp√©rience pratique dans le cadre du [processus de certification](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), vous devez envoyer votre mod√®le de taxi entra√Æn√© au Hub et **obtenir un r√©sultat >= 4,5**.\n",
        "\n",
        "Pour trouver votre r√©sultat, rendez-vous sur le [classement](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)et trouvez votre mod√®le, **le r√©sultat = r√©compense_moyenne - √©cart-type de la r√©compense**\n",
        "\n",
        "\n",
        "Pour plus d'informations sur le processus de certification, consultez cette section.\n",
        " üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ],
      "metadata": {
        "id": "Kdxb1IhzTn0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation des d√©pendances et cr√©ation d'un √©cran virtuel üîΩ\n",
        "Dans le notebook, nous devons g√©n√©rer une vid√©o de relecture.\n",
        "\n",
        "Pour ce faire, avec Colab, **nous avons besoin d'un √©cran virtuel pour afficher l'environnement** (et donc enregistrer les images). La cellule suivante installera donc les biblioth√®ques, puis cr√©era et ex√©cutera un √©cran virtuel üñ•. Nous installerons plusieurs biblioth√®ques :\n",
        "- `gymnasium` : Contient les environnements FrozenLake-v1 ‚õÑ et Taxi-v3 üöï.\n",
        "- `pygame` : Utilis√© pour l'interface utilisateur de FrozenLake-v1 et Taxi-v3.\n",
        "- `numpy` : Utilis√© pour la gestion de notre table Q.\n",
        "\n",
        "Le Hugging Face Hub ü§ó est une plateforme centrale o√π chacun peut partager et explorer des mod√®les et des jeux de donn√©es. Il propose un syst√®me de versionnage, des m√©triques, des visualisations et d'autres fonctionnalit√©s facilitant la collaboration.\n",
        "\n",
        "Vous pouvez consulter ici tous les mod√®les Deep RL disponibles (s'ils utilisent le Q-Learning).\n",
        "üëâ https://huggingface.co/models?other=q-learning"
      ],
      "metadata": {
        "id": "4gpxC1_kqUYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cette cellule est redondante et sera vid√©e. Les installations seront g√©r√©es dans la cellule n71uTX7qqzz2.\n",
        "# !pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
      ],
      "metadata": {
        "id": "VrGIL_I2sDrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl python3-dev build-essential libyaml-dev\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Installer les d√©pendances Python ici, apr√®s les d√©pendances syst√®me\n",
        "# T√©l√©charger les requirements.txt, filtrer pyyaml et pickle5, puis les installer\n",
        "# pickle5 n'est pas n√©cessaire pour Python 3.12+\n",
        "!curl -s https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt | grep -vE '^(pyyaml|pickle5)' > filtered_requirements.txt\n",
        "!pip install -r filtered_requirements.txt\n",
        "!rm filtered_requirements.txt # Nettoyer le fichier temporaire\n",
        "\n",
        "# Installer pyyaml s√©par√©ment. Ne pas sp√©cifier de version pour laisser pip trouver une version compatible ou une roue pr√©-compil√©e.\n",
        "!pip install pyyaml"
      ],
      "metadata": {
        "id": "n71uTX7qqzz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour que les nouvelles biblioth√®ques install√©es soient utilis√©es, **il est parfois n√©cessaire de red√©marrer l'environnement d'ex√©cution du notebook**. La cellule suivante provoquera un plantage de l'environnement d'ex√©cution ; vous devrez donc vous reconnecter et ex√©cuter le code √† partir d'ici. Gr√¢ce √† cette astuce, **nous pourrons utiliser notre √©cran virtuel**.\n"
      ],
      "metadata": {
        "id": "K6XC13pTfFiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "3kuZbWAkfHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "DaY1N4dBrabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Importer les packages üì¶\n",
        "En plus des biblioth√®ques install√©es, nous utilisons √©galement :\n",
        "- `random` : pour g√©n√©rer des nombres al√©atoires (utiles pour la politique epsilon-greedy).\n",
        "- `imageio` : pour g√©n√©rer une vid√©o de replay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcNvOAQlysBJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f12a6fa9"
      },
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp4-bXKIy1mQ"
      },
      "source": [
        "Nous sommes maintenant pr√™ts √† coder notre algorithme Q-Learningüî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya49aNJWVvv"
      },
      "source": [
        "# Partie 1: Frozen Lake ‚õÑ (version non glissante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "## Cr√©er et comprendre [FrozenLake environment ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
        "---\n",
        "\n",
        "üí° Une bonne pratique lorsque vous commencez √† utiliser un environnement est de consulter sa documentation.\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "Nous allons entra√Æner notre agent Q-Learning √† naviguer de l'√©tat initial (S) √† l'√©tat final (G) en se d√©pla√ßant uniquement sur les dalles gel√©es (F) et en √©vitant les trous (H). Deux tailles d'environnement sont possibles :\n",
        "- `map_name=\"4x4\"` : une grille 4x4\n",
        "- `map_name=\"8x8\"` : une grille 8x8\n",
        "\n",
        "L'environnement poss√®de deux modes :\n",
        "- `is_slippery=False` : l'agent se d√©place toujours dans la direction pr√©vue, la surface du lac gel√© √©tant non glissante (d√©terministe).\n",
        "- `is_slippery=True` : l'agent peut ne pas toujours se d√©placer dans la direction pr√©vue, la surface du lac gel√© √©tant glissante (stochastique).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaW_LHfS0PY2"
      },
      "source": [
        "Pour l'instant, restons simples avec une carte 4x4 et une surface non glissante. Nous ajoutons un param√®tre appel√© `render_mode` qui sp√©cifie le mode de visualisation de l'environnement. Dans notre cas, comme nous **souhaitons enregistrer une vid√©o de l'environnement √† la fin, nous devons d√©finir `render_mode` sur `rgb_array`.\n",
        "\n",
        "Comme expliqu√© dans la documentation.\n",
        " [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render)\n",
        "\n",
        " ‚Äúrgb_array‚Äù: Renvoie une seule image repr√©sentant l'√©tat actuel de l'environnement. Une image est un tableau np.ndarray de forme (x, y, 3) repr√©sentant les valeurs RGB d'une image de pixels de dimensions x par y.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJnb8O3y8up"
      },
      "outputs": [],
      "source": [
        "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n",
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\") # TODO use the correct parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji_UrI5l2zzn"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNxUbPMP0akP"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASNViqL4tZn"
      },
      "source": [
        "Vous pouvez cr√©er votre propre grille personnalis√©e comme ceci :\n",
        "\n",
        "\n",
        "```python\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
        "```\n",
        "\n",
        "mais nous utiliserons l'environnement par d√©faut pour le moment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXbTfdeJ1Xi9"
      },
      "source": [
        "### Voyons voir √† quoi ressemble l'environnement :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "Avec `Observation Space Shape Discrete(16)`, on constate que l'observation est un entier repr√©sentant la **position actuelle de l'agent, calcul√©e comme suit : current_row * ncols + current_col (o√π la ligne et la colonne commencent √† 0)**.\n",
        "\n",
        "Par exemple, la position cible sur une carte 4x4 se calcule ainsi : 3 * 4 + 3 = 15. Le nombre d'observations possibles d√©pend de la taille de la carte. **Par exemple, une carte 4x4 poss√®de 16 observations possibles.**\n",
        "\n",
        "Voici, par exemple, √† quoi ressemble l'√©tat = 0 :\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "L'espace d'actions (l'ensemble des actions possibles de l'agent) est discret et comprend 4 actions :\n",
        "- 0 : ALLER √Ä GAUCHE\n",
        "- 1 : ALLER EN BAS\n",
        "- 2 : ALLER √Ä DROITE\n",
        "- 3 : ALLER EN HAUT\n",
        "\n",
        "Fonction de r√©compense :\n",
        "- Objectif atteint : +1\n",
        "- Trou atteint : 0\n",
        "- Zone gel√©e atteinte : 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "## Cr√©ation et initialisation de la table Q üóÑÔ∏è\n",
        " (üëÄ √âtape 1 du pseudocode)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "\n",
        "Il est temps d'initialiser notre table Q ! Pour d√©terminer le nombre de lignes (√©tats) et de colonnes (actions) √† utiliser, nous devons conna√Ætre l'espace des actions et des observations. Nous connaissons d√©j√† leurs valeurs, mais nous souhaitons les obtenir par programmation afin que notre algorithme soit g√©n√©ralisable √† diff√©rents environnements. Gym nous offre cette possibilit√© : `env.action_space.n` et `env.observation_space.n`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3ZCdluj3k0l"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCddoOXM3UQH"
      },
      "outputs": [],
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YfvrqRt3jdR"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67OdoKL63eDD"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuTKv3th3ohG"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnrb_nX33fJo"
      },
      "outputs": [],
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0WlgkVO3Jf9"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "## D√©finir la politique avide\n",
        "\n",
        "\n",
        "N'oubliez pas que nous avons deux politiques, car l'apprentissage par renforcement Q-Learning est un algorithme **hors strat√©gie**. Cela signifie que nous utilisons une politique diff√©rente pour l'action et la mise √† jour de la fonction de valeur.\n",
        "- Politique epsilon-gloutonne (politique d'action)\n",
        "- Politique gloutonne (politique de mise √† jour)\n",
        "\n",
        "La politique gloutonne sera √©galement la politique finale une fois l'entra√Ænement de l'agent Q-Learning termin√©. Elle sert √† s√©lectionner une action √† l'aide de la table Q.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3SCLmLX5bWG"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action =\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien s√ªr, je peux vous expliquer cette commande :\n",
        "\n",
        "**np.argmax(Qtable[state][:])**\n",
        "\n",
        "Cette commande est utilis√©e pour trouver la meilleure action √† prendre dans un √©tat donn√©, selon la table Q (Qtable).\n",
        "\n",
        "Voici une d√©composition :\n",
        "\n",
        "**Qtable** : C'est votre table Q, une matrice o√π les lignes repr√©sentent les √©tats et les colonnes repr√©sentent les actions. Chaque cellule Qtable[√©tat][action] contient une valeur qui estime la r√©compense future attendue si l'agent prend action depuis l'√©tat donn√©.\n",
        "\n",
        "**Qtable[state]** : Cela s√©lectionne la ligne de la table Q qui correspond √† l'√©tat actuel de l'agent. Cette ligne contient toutes les valeurs Q pour chaque action possible depuis cet √©tat.\n",
        "\n",
        "**Qtable[state][:]** : Le [:] est facultatif ici et sert √† expliciter que vous prenez tous les √©l√©ments de la ligne s√©lectionn√©e (c'est-√†-dire toutes les valeurs Q pour toutes les actions possibles depuis cet √©tat).\n",
        "\n",
        "**np.argmax(...)** : C'est une fonction de la biblioth√®que NumPy qui renvoie l'indice de l'√©l√©ment maximal dans un tableau. Dans ce cas, elle trouve l'indice (le num√©ro de l'action) qui correspond √† la plus grande valeur Q dans la ligne de l'√©tat actuel.\n",
        "\n",
        "En r√©sum√©, cette commande permet √† l'agent de choisir l'action qui, d'apr√®s sa table Q actuelle, maximisera sa r√©compense future attendue pour l'√©tat donn√©. C'est ce qu'on appelle une action \"gourmande\" (greedy action)."
      ],
      "metadata": {
        "id": "zWO1eVZTqumx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_-8b8z5k54"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se2OzWGW5kYJ"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "##D√©finition de la politique epsilon-greedy ü§ñ\n",
        "La politique epsilon-greedy g√®re le compromis exploration/exploitation.\n",
        "\n",
        "Principe de la politique epsilon-greedy :\n",
        "- Avec une probabilit√© de 1 √† Œµ : **nous privil√©gions l‚Äôexploitation** (c‚Äôest-√†-dire que notre agent choisit l‚Äôaction avec la plus grande valeur pour la paire √©tat-action).\n",
        "- Avec une probabilit√© Œµ : **nous explorons** (nous essayons une action al√©atoire).\n",
        "\n",
        "Au fur et √† mesure de l‚Äôentra√Ænement, **nous r√©duisons progressivement la valeur d‚Äôepsilon, car nous aurons de moins en moins besoin d‚Äôexploration et de plus en plus besoin d‚Äôexploitation.**\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bj7x3in3_Pq"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()# Take a random action\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5ej1fS4P2V"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYxHuckr4LiG"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## D√©finissons les hyperparam√®tres ‚öôÔ∏è\n",
        "Les hyperparam√®tres li√©s √† l'exploration sont parmi les plus importants.\n",
        "- Il est essentiel que notre agent **explore suffisamment l'espace d'√©tats** pour apprendre une bonne approximation de la valeur. Pour cela, une d√©croissance progressive d'epsilon est n√©cessaire.\n",
        "- Si la d√©croissance d'epsilon est trop rapide (taux de d√©croissance trop √©lev√©), **l'agent risque de se retrouver bloqu√©**, car il n'aura pas suffisamment explor√© l'espace d'√©tats et ne pourra donc pas r√©soudre le probl√®me.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bN8QLSsWti8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "## Cr√©er la m√©thode de boucle d'entra√Ænement\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "La boucle d'entra√Ænement se d√©roule ainsi :\n",
        "\n",
        "```Pour un √©pisode dans le total des √©pisodes d'entra√Ænement :\n",
        "\n",
        "R√©duire epsilon (puisque l'exploration n√©cessaire diminue progressivement).  \n",
        "R√©initialiser l'environnement\n",
        "\n",
        "  Pour une √©tape dans le nombre maximal d'it√©rations :\n",
        "    Choisir l'action At en utilisant une politique epsilon-gloutonne\n",
        "    Effectuer l'action (a) et observer l'√©tat r√©sultant (s') et la r√©compense (r)\n",
        "    Mettre √† jour la Q-valeur Q(s,a) √† l'aide de l'√©quation de Bellman :\n",
        "    Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    Si termin√©, terminer l'√©pisode\n",
        "    Notre prochain √©tat est le nouvel √©tat\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paOynXy3aoJW"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est une excellente question, car tqdm est tr√®s utile dans les boucles d'entra√Ænement !\n",
        "\n",
        "D√©cortiquons l'expression `tqdm(range(n_training_episodes))` :\n",
        "\n",
        "- `range(n_training_episodes)` :\n",
        "\n",
        "- `n_training_episodes` est une variable (d√©finie pr√©c√©demment comme un hyperparam√®tre, par exemple 10000) qui repr√©sente le nombre total d'√©pisodes d'entra√Ænement que votre agent Q-Learning va effectuer.\n",
        "\n",
        "- `range(n_training_episodes)` g√©n√®re une s√©quence de nombres entiers, de 0 √† n_training_episodes - 1. C'est l'it√©rateur classique que vous utiliseriez dans une boucle for.\n",
        "\n",
        "`tqdm(...)` :\n",
        "\n",
        "- `tqdm` est une biblioth√®que Python qui permet d'afficher une barre de progression intelligente et rapide pour vos boucles.\n",
        "- Le nom tqdm vient de l'arabe \"taqaddum\" (ÿ™ŸÇÿØŸëŸÖ) qui signifie \"progr√®s\".\n",
        "- Lorsque vous enveloppez un it√©rateur (comme range(...)) avec tqdm, il transforme cet it√©rateur en un autre it√©rateur qui, en plus de fournir les √©l√©ments de la s√©quence, affiche et met √† jour une barre de progression dans votre console ou votre notebook.\n",
        "\n",
        "**En r√©sum√©** : tqdm(range(n_training_episodes)) va ex√©cuter une boucle for pendant n_training_episodes it√©rations, et √† chaque it√©ration, il affichera une barre de progression en temps r√©el qui vous montrera o√π vous en √™tes dans l'entra√Ænement (pourcentage, temps √©coul√©, temps restant estim√©, vitesse d'it√©ration, etc.).\n",
        "\n",
        "C'est particuli√®rement utile pour les boucles longues, car cela vous donne une indication visuelle du temps restant et du bon fonctionnement de votre code, au lieu de rester bloqu√© sur un √©cran sans retour visuel."
      ],
      "metadata": {
        "id": "ZIcwnJxavo-g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnpk2ePoem3r"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyZaYbUAeolw"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## Entra√Æner l'agent Q-Learning üèÉ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "## Voyons voir √† quoi ressemble notre tableau Q-Learning maintenant üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmfchsTITw4q"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "## M√©thode d'√©valuation üìù\n",
        "- Nous avons d√©fini la m√©thode d'√©valuation que nous allons utiliser pour tester notre agent Q-Learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param max_steps: Maximum number of steps per episode\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "## √âvaluez notre agent Q-Learning üìà\n",
        "- En g√©n√©ral, vous devriez obtenir une r√©compense moyenne de 1,0.\n",
        "- L'**environnement est relativement simple** car l'espace d'√©tats est tr√®s r√©duit (16).\n",
        "\n",
        "Voici ce que vous pouvez essayer :\n",
        "[pour la remplacer par la version glissante](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), ce qui introduit de la stochasticit√©, rendant l'environnement plus complexe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAgB7s0HEFMm"
      },
      "outputs": [],
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxaP3bPdg1DV"
      },
      "source": [
        "## Publiez notre mod√®le entra√Æn√© sur le Hub\n",
        "üî• Maintenant que nous avons constat√© de bons r√©sultats apr√®s l'entra√Ænement, **nous pouvons publier notre mod√®le entra√Æn√© sur le Hub ü§ó en une seule ligne de code**.\n",
        "\n",
        "Voici un exemple de fiche de mod√®le :\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Model card\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv0k1JQjpMq3"
      },
      "source": [
        "En coulisses, le Hub utilise des d√©p√¥ts bas√©s sur git (ne vous inqui√©tez pas si vous ne savez pas ce qu'est git), ce qui signifie que vous pouvez mettre √† jour le mod√®le avec de nouvelles versions au fur et √† mesure que vous exp√©rimentez et am√©liorez votre agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ5LrR-joIHD"
      },
      "source": [
        "#### Do not modify this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jex3i9lZ8ksX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo57HBn3W74O"
      },
      "outputs": [],
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  state, info = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render()\n",
        "  images.append(img)\n",
        "  while not terminated or truncated:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def push_to_hub(\n",
        "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
        "    This method does the complete pipeline:\n",
        "    - It evaluates the model\n",
        "    - It generates the model card\n",
        "    - It generates a replay video of the agent\n",
        "    - It pushes everything to the Hub\n",
        "\n",
        "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param env\n",
        "    :param video_fps: how many frame per seconds to record our video replay\n",
        "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    :param local_repo_path: where the local repository is\n",
        "    \"\"\"\n",
        "    _, repo_name = repo_id.split(\"/\")\n",
        "\n",
        "    eval_env = env\n",
        "    api = HfApi()\n",
        "\n",
        "    # Step 1: Create the repo\n",
        "    repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "\n",
        "    # Step 2: Download files\n",
        "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
        "\n",
        "    # Step 3: Save the model\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
        "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "            model[\"slippery\"] = False\n",
        "\n",
        "    # Pickle the model\n",
        "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
        "    mean_reward, std_reward = evaluate_agent(\n",
        "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
        "    )\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": model[\"env_id\"],\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
        "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Write a JSON file called \"results.json\" that will contain the\n",
        "    # evaluation results\n",
        "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 5: Create the model card\n",
        "    env_name = model[\"env_id\"]\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "        env_name += \"-\" + \"no_slippery\"\n",
        "\n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
        "\n",
        "    # Add metrics\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "    )\n",
        "\n",
        "    # Merges both dictionaries\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Q-Learning** Agent playing1 **{env_id}**\n",
        "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
        "\n",
        "  ## Usage\n",
        "\n",
        "  ```python\n",
        "\n",
        "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
        "\n",
        "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
        "  env = gym.make(model[\"env_id\"])\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
        "\n",
        "    readme_path = repo_local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    print(readme_path.exists())\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # Step 6: Record a video\n",
        "    video_path = repo_local_path / \"replay.mp4\"\n",
        "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
        "\n",
        "    # Step 7. Push everything to the Hub\n",
        "    api.upload_folder(\n",
        "        repo_id=repo_id,\n",
        "        folder_path=repo_local_path,\n",
        "        path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
      ],
      "metadata": {
        "id": "U4mdUTKkGnUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81J6cet_ogSS"
      },
      "source": [
        "### .\n",
        "\n",
        "En utilisant `push_to_hub`, **vous √©valuez, enregistrez une rediffusion, g√©n√©rez une fiche mod√®le de votre agent et la transf√©rez sur le Hub**. Ainsi :\n",
        "- Vous pouvez **pr√©senter votre travail** üî•\n",
        "- Vous pouvez **visualiser votre agent en action** üëÄ\n",
        "- Vous pouvez **partager un agent avec la communaut√© pour que d'autres puissent l'utiliser** üíæ\n",
        "- Vous pouvez **acc√©der √† un classement üèÜ pour comparer les performances de votre agent √† celles de vos camarades**\n",
        " üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnFC0iZooTw"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB5nIcxR8paT"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWc1x3-o3xG"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login` (or `login`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5AfUeFo3xH"
      },
      "source": [
        "3Ô∏è‚É£ Nous sommes maintenant pr√™ts √† d√©ployer notre agent entra√Æn√© sur le Hub ü§ó üî• gr√¢ce √† la fonction `push_to_hub()`.\n",
        "- Cr√©ons **le dictionnaire du mod√®le contenant les hyperparam√®tres et la table Q_table**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiMqxqVHg0I4"
      },
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_frozenlake\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kld-AEso3xH"
      },
      "source": [
        "Let's fill the `push_to_hub` function:\n",
        "\n",
        "- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `\n",
        "(repo_id = {username}/{repo_name})`\n",
        "üí° A good `repo_id` is `{username}/q-{env_id}`\n",
        "- `model`: our model dictionary containing the hyperparameters and the Qtable.\n",
        "- `env`: the environment.\n",
        "- `commit_message`: message of the commit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sBo2umnXpPd"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpOTtSt83kPZ"
      },
      "outputs": [],
      "source": [
        "username = \"metapat973\" # FILL THIS\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2875IGsprzq"
      },
      "source": [
        "F√©licitations ü•≥ ! Vous venez de cr√©er, d'entra√Æner et de d√©ployer votre premier agent d'apprentissage par renforcement.\n",
        "\n",
        "L'environnement FrozenLake-v1 no_slippery est tr√®s simple ; essayons-en un plus complexe üî•.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18lN8Bz7yvLt"
      },
      "source": [
        "# Part 2: Taxi-v3 üöñ\n",
        "\n",
        "## Cr√©er et comprendre [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "---\n",
        "\n",
        "üí° A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n",
        "\n",
        "---\n",
        "\n",
        "Dans `Taxi-v3` üöï, quatre emplacements sont d√©sign√©s dans le monde quadrill√©, indiqu√©s par\n",
        "- R (rouge),\n",
        "- G (vert),\n",
        "- Y (jaune) et\n",
        "- B (bleu).\n",
        "\n",
        "Au d√©but de l'√©pisode, **le taxi d√©marre d'une case al√©atoire** et le passager se trouve √† un emplacement al√©atoire. Le taxi se rend √† l'emplacement du passager, **le prend en charge**, puis le conduit √† sa destination (un autre des quatre emplacements d√©sign√©s), et **le d√©pose**. Une fois le passager d√©pos√©, l'√©pisode se termine.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL0wpeO8gpej"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBOaXgtsrmtT"
      },
      "source": [
        "Il existe **500 √©tats discrets**, car il y a 25 positions de taxi, 5 emplacements possibles pour le passager (y compris le cas o√π le passager se trouve dans le taxi) et **4 destinations possibles**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TPNaGSZrgqA"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdeeZuokrhit"
      },
      "outputs": [],
      "source": [
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1r50Advrh5Q"
      },
      "source": [
        "**L'espace d'actions** (l'ensemble des actions possibles de l'agent) est discret et comprend 6 actions :\n",
        "- 0 : se d√©placer vers le sud\n",
        "- 1 : se d√©placer vers le nord\n",
        "- 2 : se d√©placer vers l'est\n",
        "- 3 : se d√©placer vers l'ouest\n",
        "- 4 : prendre en charge un passager\n",
        "- 5 : d√©poser un passager\n",
        "\n",
        "**Fonction de r√©compense** :\n",
        "- -1 par √©tape, sauf si une autre r√©compense est d√©clench√©e.\n",
        "- +20 pour chaque passager d√©pos√©.\n",
        "- -10 pour chaque prise en charge ou d√©pose effectu√©e ill√©galement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US3yDXnEtY9I"
      },
      "outputs": [],
      "source": [
        "# Create our Q table with state_size rows and action_size columns (500x6)\n",
        "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
        "print(Qtable_taxi)\n",
        "print(\"Q-table shape: \", Qtable_taxi .shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUMKPH0_LJyH"
      },
      "source": [
        "## D√©finissez les hyperparam√®tres ‚öôÔ∏è\n",
        "‚ö† NE MODIFIEZ PAS EVAL_SEED : le tableau eval_seed **nous permet d‚Äô√©valuer votre agent avec les m√™mes positions de d√©part de taxi pour chaque √©l√®ve**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB6n__hhg7YS"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 25000   # Total training episodes\n",
        "learning_rate = 0.7           # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# DO NOT MODIFY EVAL_SEED\n",
        "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
        " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
        " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
        "                                                          # Each seed has a specific starting state\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"Taxi-v3\"           # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05           # Minimum exploration probability\n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TMORo1VLTsX"
      },
      "source": [
        "## Former notre agent Q-Learning üèÉ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwP3Y2z2eS-K"
      },
      "outputs": [],
      "source": [
        "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
        "Qtable_taxi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPdu0SueLVl2"
      },
      "source": [
        "## Cr√©ation d'un dictionnaire de mod√®le üíæ et publication de notre mod√®le entra√Æn√© sur le Hub üî•\n",
        "- Nous cr√©ons un dictionnaire de mod√®le contenant tous les hyperparam√®tres d'entra√Ænement pour la reproductibilit√© et la table Q."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a1FpE_3hNYr"
      },
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_taxi\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhQtiQozhOn1"
      },
      "outputs": [],
      "source": [
        "username = \"metapat973\" # FILL THIS\n",
        "repo_name = \"Taxi-v3\" # FILL THIS\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgSdjgbIpRti"
      },
      "source": [
        "Maintenant que votre Taxi-v3 est disponible sur le Hub, vous pouvez comparer ses r√©sultats avec ceux de vos camarades de classe gr√¢ce au classement.\n",
        " üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi Leaderboard\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzgIO70c0bu2"
      },
      "source": [
        "# Partie 3 : Charger depuis le Hub üîΩ\n",
        "Ce qui est g√©nial avec Hugging Face Hub ü§ó, c‚Äôest que vous pouvez facilement charger des mod√®les performants issus de la communaut√©.\n",
        "\n",
        "Charger un mod√®le enregistr√© depuis le Hub est tr√®s simple :\n",
        "\n",
        "\n",
        "1. Vous allez √† https://huggingface.co/models?other=q-learning pour voir la liste de tous les mod√®les sauvegard√©s de q-learning.\n",
        "2. Vous en selectionner un et copyier son repo_id\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"Copy id\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTth6thRoC6X"
      },
      "source": [
        "3. Il suffit ensuite d'utiliser `load_from_hub` avec :\n",
        "- L'identifiant du d√©p√¥t (repo_id)\n",
        "- Le nom du fichier : le mod√®le enregistr√© dans le d√©p√¥t.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrfoTaBoNrd"
      },
      "source": [
        "#### Do not modify this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo8qEzNtCaVI"
      },
      "outputs": [],
      "source": [
        "from urllib.error import HTTPError\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "def load_from_hub(repo_id: str, filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Download a model from Hugging Face Hub.\n",
        "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param filename: name of the model zip file from the repository\n",
        "    \"\"\"\n",
        "    # Get the model from the Hub, download and cache the model on your local disk\n",
        "    pickle_model = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=filename\n",
        "    )\n",
        "\n",
        "    with open(pickle_model, 'rb') as f:\n",
        "      downloaded_model_file = pickle.load(f)\n",
        "\n",
        "    return downloaded_model_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_sM2gNioPZH"
      },
      "source": [
        "### ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUm9lz2gCQcU"
      },
      "outputs": [],
      "source": [
        "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n",
        "\n",
        "print(model)\n",
        "env = gym.make(model[\"env_id\"])\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7pL8rg1MulN"
      },
      "outputs": [],
      "source": [
        "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n",
        "\n",
        "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQAwLnYFPk-s"
      },
      "source": [
        "## Quelques d√©fis suppl√©mentaires üèÜ\n",
        "Le meilleur moyen d'apprendre, c'est de pratiquer par soi-m√™me ! Comme vous l'avez constat√©, l'agent actuel n'est pas tr√®s performant.\n",
        "\n",
        "Pour commencer, nous vous sugg√©rons de vous entra√Æner √† effectuer davantage de pas. Avec 1 000 000 de pas, nous avons obtenu d'excellents r√©sultats !\n",
        "Dans le [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) Vous trouverez vos agents. Parviendrez-vous √† atteindre le sommet ?\n",
        "\n",
        "Voici quelques id√©es pour grimper au classement :\n",
        "* Augmentez le nombre d'√©tapes d'entra√Ænement.\n",
        "* Testez diff√©rents hyperparam√®tres en vous inspirant des mod√®les de vos camarades.\n",
        "* **Soumettez votre nouveau mod√®le entra√Æn√©** sur le Hub üî•\n",
        "\n",
        "AVous trouvez la marche sur la glace et la conduite de taxis trop ennuyeuses ? Essayez de **changer d'environnement**, pourquoi ne pas essayer la version glissante de FrozenLake-v1 ?\n",
        "\n",
        "Check how they work [using the gymnasium documentation](https://gymnasium.farama.org/) and have fun üéâ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fW-EU5WejJ"
      },
      "source": [
        "_____________________________________________________________________\n",
        "F√©licitations ü•≥, vous venez d'impl√©menter, d'entra√Æner et de d√©ployer votre premier agent d'apprentissage par renforcement.\n",
        "\n",
        "Comprendre le Q-Learning est une **√©tape importante pour comprendre les m√©thodes bas√©es sur la valeur**.\n",
        "\n",
        "Dans la prochaine unit√© consacr√©e au Deep Q-Learning, nous verrons que si la cr√©ation et la mise √† jour d'une table Q √©taient une bonne strat√©gie, **elle n'est cependant pas √©volutive**. Par exemple, imaginez que vous cr√©iez un agent qui apprend √† jouer √† Doom.\n",
        "\n",
        "\n",
        "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
        "\n",
        "Doom est un environnement vaste avec un espace d'√©tats immense (des millions d'√©tats diff√©rents). Cr√©er et mettre √† jour une table Q pour cet environnement serait inefficace.\n",
        "\n",
        "C'est pourquoi, dans la prochaine unit√©, nous √©tudierons l'apprentissage par renforcement profond (Deep Q-Learning), un algorithme qui utilise un r√©seau de neurones pour approximer, √©tant donn√© un √©tat, les diff√©rentes valeurs Q pour chaque action.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjLhT70TEZIn"
      },
      "source": [
        "See you in Unit 3! üî•\n",
        "\n",
        "## Keep learning, stay awesome ü§ó"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "67OdoKL63eDD",
        "B2_-8b8z5k54",
        "8R5ej1fS4P2V",
        "Pnpk2ePoem3r"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}