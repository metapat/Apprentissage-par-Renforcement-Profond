{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metapat/Apprentissage-par-Renforcement-Profond/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3NL_e4crQv"
      },
      "source": [
        "# Bonus Unit√© 1 : Apprenons √† Huggy le chien üê∂ √† rapporter un b√¢ton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYrDriDujzX"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit2/thumbnail.png\" alt=\"Bonus Unit 1Thumbnail\">\n",
        "\n",
        "Dans ce cahier, nous consoliderons les acquis de la premi√®re unit√© en **apprenant √† Huggy le chien √† rapporter le b√¢ton, puis en jouant directement avec lui dans votre navigateur**.\n",
        "\n",
        "‚¨áÔ∏è Voici un exemple de ce que **vous serez capable de faire √† la fin de cette unit√©**. ‚¨áÔ∏è (cliquez sur ‚ñ∂ pour voir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnVhs1yYNyUF"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7oR6R-ZIbeS"
      },
      "source": [
        "### L'environnement üéÆ\n",
        "- Huggy le chien, un environnement cr√©√© par [Thomas Simonini](https://twitter.com/ThomasSimonini) based on [Puppo The Corgi](https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit)\n",
        "\n",
        "### La biblioth√®que utilis√©e üìö\n",
        "\n",
        "- [MLAgents](https://github.com/Unity-Technologies/ml-agents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60yACvZwO0Cy"
      },
      "source": [
        "Nous essayons constamment d'am√©liorer nos tutoriels, donc **si vous trouvez des probl√®mes dans ce notebook**, veuillez [ouvrir un probl√®me sur le d√©p√¥t Github](https://github.com/huggingface/deep-rl-class/issues).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oks-ETYdO2Dc"
      },
      "source": [
        "## Objectifs de ce cahier üèÜ\n",
        "\n",
        "√Ä la fin de ce cahier, vous serez capable de :\n",
        "- Comprendre **l‚Äôespace d‚Äô√©tats, l‚Äôespace d‚Äôactions et la fonction de r√©compense utilis√©s pour entra√Æner Huggy**.\n",
        "- **Entra√Æner votre propre Huggy** √† rapporter le b√¢ton.\n",
        "- Jouer **directement avec votre Huggy entra√Æn√© dans votre navigateur**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUlVrqnBv2o1"
      },
      "source": [
        "## Ce carnet provient du cours d'apprentissage par renforcement profond.\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAMjaQpHwB_s"
      },
      "source": [
        "Dans ce cours gratuit, vous allez :\n",
        "- üìñ √âtudier l‚Äôapprentissage par renforcement profond, tant sur le plan th√©orique que pratique.\n",
        "- üßë‚Äçüíª Apprendre √† utiliser des biblioth√®ques d‚Äôapprentissage par renforcement profond reconnues, telles que Stable Baselines3, RL Baselines3 Zoo, CleanRL et Sample Factory 2.0.\n",
        "- ü§ñ Entra√Æner des agents dans des environnements uniques.\n",
        "\n",
        "Pour en savoir plus, consultez le programme. üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "Le meilleur moyen de rester en contact est de rejoindre notre serveur Discord pour √©changer avec la communaut√© et avec nous üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r7Hl0uywFSO"
      },
      "source": [
        "## Pr√©requis üèóÔ∏è\n",
        "Avant de commencer, vous devez :   \n",
        "üî≤ üìö **Comprendre les fondements de l‚Äôapprentissage par renforcement** (module critique, d√©pendance temporelle, hypoth√®se des r√©compenses‚Ä¶) en r√©alisant l‚Äôunit√© 1.   \n",
        "üî≤ üìö **Lire l‚Äôintroduction de Huggy** en r√©alisant l‚Äôunit√© bonus 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DssdIjk_8vZE"
      },
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfCXHy68xBv"
      },
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an3ByrXYQ4iK"
      },
      "source": [
        "## Cloner le d√©p√¥t üîΩ\n",
        "- Nous devons cloner le d√©p√¥t contenant les **agents ML**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WNoL04M7rTa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Clone the repository (can take 3min)\n",
        "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kohJPf8u70aK"
      },
      "source": [
        "## Configuration de l'environnement virtuel üîΩ\n",
        "- Pour que **ML-Agents** fonctionne correctement dans Colab, la version de Python utilis√©e par Colab doit respecter les exigences de la biblioth√®que.\n",
        "- Vous pouvez v√©rifier la version de Python requise gr√¢ce au param√®tre `python_requires` dans les fichiers `setup.py`. Ces fichiers sont n√©cessaires √† la configuration de la biblioth√®que **ML-Agents** et se trouvent aux emplacements suivants :\n",
        "  - `/content/ml-agents/ml-agents/setup.py`\n",
        "  - `/content/ml-agents/ml-agents-envs/setup.py`\n",
        "- La version Python actuelle de Colab (v√©rifiable avec `!python --version`) ne correspond pas au param√®tre `python_requires` de la biblioth√®que. Par cons√©quent, l'installation peut √©chouer silencieusement et entra√Æner des erreurs comme celles-ci lors de l'ex√©cution ult√©rieure des m√™mes commandes :\n",
        "  - `/bin/bash: line 1: mlagents-learn: command not found`\n",
        "  - `/bin/bash: line 1: mlagents-push-to-hf: command not found`\n",
        "\n",
        "- Pour r√©soudre ce probl√®me, nous allons cr√©er un environnement virtuel avec une version de Python compatible avec la biblioth√®que **ML-Agents**.\n",
        "\n",
        "Remarque : *Pour assurer la compatibilit√© future, v√©rifiez toujours le param√®tre `python_requires` dans les fichiers d'installation et configurez votre environnement virtuel avec la version maximale de Python prise en charge dans le script ci-dessous si la version de Python de Colab n'est pas compatible.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikk36JNn70aK"
      },
      "outputs": [],
      "source": [
        "# Colab's Current Python Version (Incompatible with ML-Agents)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWJtlkyR70aK"
      },
      "outputs": [],
      "source": [
        "# Download and install Miniconda\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!./miniconda.sh -b -f -p /usr/local\n",
        "\n",
        "# Accept Conda Terms of Service and disable prompts\n",
        "!/usr/local/bin/conda config --set always_yes yes\n",
        "!/usr/local/bin/conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\n",
        "!/usr/local/bin/conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r\n",
        "\n",
        "# Create a new conda environment for Python 3.10.12\n",
        "!/usr/local/bin/conda create -n mlagents_env python=3.10.12 ujson -y\n",
        "\n",
        "# Manually update PATH for the current session to include the new conda environment\n",
        "import os\n",
        "os.environ['PATH'] = f\"/usr/local/envs/mlagents_env/bin:{os.environ['PATH']}\"\n",
        "os.environ['CONDA_PREFIX'] = \"/usr/local/envs/mlagents_env\"\n",
        "os.environ['PYTHONPATH'] = f\"/usr/local/envs/mlagents_env/lib/python3.10/site-packages:{os.environ.get('PYTHONPATH', '')}\"\n",
        "\n",
        "# Verify Python version in the new environment (should be 3.10.12)\n",
        "!python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhE7brD770aK"
      },
      "outputs": [],
      "source": [
        "# Python Version in New Virtual Environment (Compatible with ML-Agents)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r60nFOdm70aK"
      },
      "source": [
        "## Installation des d√©pendances üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8wmVcMk7xKo"
      },
      "outputs": [],
      "source": [
        "# Install the package using absolute paths (can take 3min)\n",
        "!/usr/local/bin/conda run -n mlagents_env pip install -e /content/ml-agents/ml-agents-envs\n",
        "!/usr/local/bin/conda run -n mlagents_env pip install -e /content/ml-agents/ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRY5ufKUKfhI"
      },
      "source": [
        "### T√©l√©charger et d√©placer le fichier zip de l'environnement dans\n",
        "`./trained-envs-executables/linux/`\n",
        "\n",
        "- Notre fichier ex√©cutable d'environnement se trouve dans une archive zip.\n",
        "- Il faut le t√©l√©charger et le placer dans le r√©pertoire appropri√©.\n",
        " `./trained-envs-executables/linux/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Ls6_6eOKiA"
      },
      "outputs": [],
      "source": [
        "!mkdir ./trained-envs-executables\n",
        "!mkdir ./trained-envs-executables/linux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHh_LXsRrrbM"
      },
      "source": [
        "Nous avons t√©l√©charg√© le fichier Huggy.zip depuis https://github.com/huggingface/Huggy en utilisant `wget`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xNAD1tRpy0_"
      },
      "outputs": [],
      "source": [
        "!wget \"https://github.com/huggingface/Huggy/raw/main/Huggy.zip\" -O ./trained-envs-executables/linux/Huggy.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FPx0an9IAwO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./trained-envs-executables/linux/ ./trained-envs-executables/linux/Huggy.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyumV5XfPKzu"
      },
      "source": [
        "Assurez-vous que votre fichier est accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdFsLJ11JvQf"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./trained-envs-executables/linux/Huggy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYKVj8yUvj55"
      },
      "source": [
        "## R√©capitulons le fonctionnement de cet environnement\n",
        "### L'espace d'√©tats : ce que Huggy ¬´ per√ßoit ¬ª. Huggy ne ¬´ voit ¬ª pas son environnement. Nous lui fournissons plut√¥t des informations √† son sujet :\n",
        "- La position de la cible (le b√¢ton)\n",
        "- Sa position relative par rapport √† la cible\n",
        "- L'orientation de ses jambes.\n",
        "\n",
        "Gr√¢ce √† ces informations, Huggy **peut d√©cider de la prochaine action √† entreprendre pour atteindre son objectif**.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.jpg\" alt=\"Huggy\" width=\"100%\">\n",
        "\n",
        "### L'espace d'action : les mouvements que Huggy peut faire\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-action.jpg\" alt=\"Huggy action\" width=\"100%\">\n",
        "\n",
        "Les jambes de Huggy sont actionn√©es par des moteurs articulaires. Cela signifie que pour atteindre la cible, Huggy doit apprendre √† faire pivoter correctement les moteurs articulaires de chacune de ses jambes afin de pouvoir se d√©placer.\n",
        "\n",
        "### La fonction de r√©compense\n",
        "\n",
        "La fonction de r√©compense est con√ßue pour que **Huggy atteigne son objectif** : rapporter le b√¢ton.\n",
        "\n",
        "Rappelons que l‚Äôun des fondements de l‚Äôapprentissage par renforcement est l‚Äô*hypoth√®se de r√©compense* : un objectif peut √™tre d√©crit comme la **maximisation de la r√©compense cumul√©e attendue**.\n",
        "\n",
        "Ici, notre objectif est que Huggy **se dirige vers le b√¢ton sans trop tourner sur lui-m√™me**. Par cons√©quent, notre fonction de r√©compense doit traduire cet objectif. Notre fonction de r√©compense :\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/reward.jpg\" alt=\"Huggy reward function\" width=\"100%\">\n",
        "\n",
        "- *Bonus d'orientation* : nous le **r√©compensons lorsqu'il s'approche de la cible.**\n",
        "- *P√©nalit√© de temps* : une p√©nalit√© de temps fixe est appliqu√©e √† chaque action afin de **l'obliger √† atteindre le b√¢ton le plus rapidement possible**.\n",
        "- *P√©nalit√© de rotation* : nous p√©nalisons Huggy **s'il tourne trop vite sur lui-m√™me.**\n",
        "- *R√©compense pour avoir atteint la cible* : nous r√©compensons Huggy **lorsqu'il atteint la cible.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuEq32Mwvtz"
      },
      "source": [
        "## Cr√©ez le fichier de configuration Huggy.\n",
        "- Dans ML-Agents, vous d√©finissez les **hyperparam√®tres d'entra√Ænement** dans des fichiers config.yaml.\n",
        "- Dans le cadre de ce notebook, nous n'allons pas modifier les hyperparam√®tres. Toutefois, si vous souhaitez faire des essais, vous pouvez modifier d'autres hyperparam√®tres. Unity fournit une documentation tr√®s compl√®te expliquant chacun d'eux [disponible ici](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
        "\n",
        "- Il nous faut cr√©er un fichier de configuration pour Huggy.\n",
        "  - Pour ce faire, cliquez sur l'ic√¥ne du dossier √† gauche de votre √©cran.\n",
        "\n",
        "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/create_file.png\" alt=\"Create file\" width=\"10%\">\n",
        "\n",
        "  - Allez √† `/content/ml-agents/config/ppo`\n",
        "  - Faites un clic droit et cr√©ez un nouveau fichier appel√© `Huggy.yaml`\n",
        "\n",
        "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/create-huggy.png\" alt=\"Create huggy.yaml\" width=\"20%\">\n",
        "\n",
        "- Copiez et collez le contenu ci-dessous üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loQ0N5jhXW71"
      },
      "outputs": [],
      "source": [
        "behaviors:\n",
        "  Huggy:\n",
        "    trainer_type: ppo\n",
        "    hyperparameters:\n",
        "      batch_size: 2048\n",
        "      buffer_size: 20480\n",
        "      learning_rate: 0.0003\n",
        "      beta: 0.005\n",
        "      epsilon: 0.2\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "      learning_rate_schedule: linear\n",
        "    network_settings:\n",
        "      normalize: true\n",
        "      hidden_units: 512\n",
        "      num_layers: 3\n",
        "      vis_encode_type: simple\n",
        "    reward_signals:\n",
        "      extrinsic:\n",
        "        gamma: 0.995\n",
        "        strength: 1.0\n",
        "    checkpoint_interval: 200000\n",
        "    keep_checkpoints: 15\n",
        "    max_steps: 2e6\n",
        "    time_horizon: 1000\n",
        "    summary_freq: 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oakN7UHwXdCX"
      },
      "source": [
        "-N'oubliez pas d'enregistrer le fichier !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9wv5NYGw-05"
      },
      "source": [
        "- **Si vous souhaitez modifier les hyperparam√®tres**, dans Google Colab Notebook, vous pouvez cliquer ici pour ouvrir le fichier config.yaml :\n",
        " `/content/ml-agents/config/ppo/Huggy.yaml`\n",
        "\n",
        "- Par exemple, **si vous souhaitez enregistrer davantage de mod√®les pendant l'entra√Ænement** (actuellement, nous enregistrons tous les 200 000 pas de temps d'entra√Ænement), vous devez modifier :\n",
        "  - `checkpoint_interval`: Le nombre d'√©tapes de temps d'entra√Ænement collect√©es entre chaque point de contr√¥le.\n",
        "\n",
        "  - `keep_checkpoints`: Le nombre maximal de points de contr√¥le du mod√®le √† conserver.\n",
        "\n",
        "=> N‚Äôoubliez pas que **diminuer la valeur de `checkpoint_interval` signifie plus de mod√®les √† t√©l√©charger sur le Hub et donc un temps de t√©l√©chargement plus long**. Nous sommes maintenant pr√™ts √† entra√Æner notre agent üî•.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fI555bO12v"
      },
      "source": [
        "## Entra√Æner notre agent\n",
        "Pour entra√Æner notre agent, il suffit de **lancer mlagents-learn et de s√©lectionner le fichier ex√©cutable contenant l'environnement.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/mllearn.png\" alt=\"ml learn function\" width=\"100%\">\n",
        "\n",
        "Avec ML Agents, nous ex√©cutons un script d'entra√Ænement. Nous d√©finissons quatre param√®tres :\n",
        "1. `mlagents-learn <config>` : le chemin d'acc√®s au fichier de configuration des hyperparam√®tres.\n",
        "2. `--env` : l'emplacement de l'ex√©cutable d'environnement.\n",
        "3. `--run-id` : le nom √† attribuer √† l'identifiant d'ex√©cution de l'entra√Ænement.\n",
        "4. `--no-graphics` : d√©sactiver l'affichage des visualisations pendant l'entra√Ænement.\n",
        "\n",
        "Entra√Ænez le mod√®le et utilisez l'option `--resume` pour reprendre l'entra√Ænement en cas d'interruption.\n",
        "\n",
        "> L'entra√Ænement √©chouera la premi√®re fois que vous utiliserez `--resume`. R√©essayez d'ex√©cuter le bloc pour contourner l'erreur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN32oWF8zPjs"
      },
      "source": [
        "La formation durera de 30 √† 45 minutes en fonction de votre machine (n'oubliez pas de **configurer un GPU**), allez prendre un ‚òïÔ∏è, vous le m√©ritez ü§ó."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS-Yh1UdHfzy"
      },
      "outputs": [],
      "source": [
        "!/usr/local/bin/conda run -n mlagents_env /usr/local/envs/mlagents_env/bin/mlagents-learn /content/ml-agents/config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=\"Huggy2\" --no-graphics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbd768f7"
      },
      "source": [
        "!ls -l /usr/local/envs/mlagents_env/bin/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37f43f7f"
      },
      "source": [
        "!ls -l ./trained-envs-executables/linux/Huggy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1c3d8ee"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "config_dir = '/content/ml-agents/config/ppo'\n",
        "config_file_path = os.path.join(config_dir, 'Huggy.yaml')\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "\n",
        "# Define the content for Huggy.yaml\n",
        "config_content = \"\"\"\n",
        "behaviors:\n",
        "  Huggy:\n",
        "    trainer_type: ppo\n",
        "    hyperparameters:\n",
        "      batch_size: 2048\n",
        "      buffer_size: 20480\n",
        "      learning_rate: 0.0003\n",
        "      beta: 0.005\n",
        "      epsilon: 0.2\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "      learning_rate_schedule: linear\n",
        "    network_settings:\n",
        "      normalize: true\n",
        "      hidden_units: 512\n",
        "      num_layers: 3\n",
        "      vis_encode_type: simple\n",
        "    reward_signals:\n",
        "      extrinsic:\n",
        "        gamma: 0.995\n",
        "        strength: 1.0\n",
        "    checkpoint_interval: 200000\n",
        "    keep_checkpoints: 15\n",
        "    max_steps: 2e6\n",
        "    time_horizon: 1000\n",
        "    summary_freq: 50000\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to the Huggy.yaml file\n",
        "with open(config_file_path, 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(f\"Fichier de configuration Huggy.yaml cr√©√© √† : {config_file_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "001ff7cf"
      },
      "source": [
        "!/usr/local/bin/conda run -n mlagents_env which mlagents-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vo0egQfjLEjh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vue94AzPy1t"
      },
      "source": [
        "## D√©ploiement de l'agent sur le Hub ü§ó\n",
        "- Maintenant que notre agent est entra√Æn√©, nous sommes **pr√™ts √† le d√©ployer sur le Hub pour que vous puissiez jouer avec Huggy directement dans votre navigateur üî•.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izT6FpgNzZ6R"
      },
      "source": [
        "Pour pouvoir partager votre mod√®le avec la communaut√©, il vous reste trois √©tapes √† suivre :\n",
        "\n",
        "1Ô∏è‚É£ (Si ce n‚Äôest pas d√©j√† fait) Cr√©ez un compte sur Hugging Face ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Connectez-vous, puis enregistrez votre jeton d‚Äôauthentification depuis le site web de Hugging Face. - Cr√©ez un nouveau jeton (https://huggingface.co/settings/tokens) **avec le r√¥le d‚Äô√©criture**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n",
        "\n",
        "- Copiez le jeton.\n",
        "- Ex√©cutez la cellule ci-dessous et collez le jeton.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b131af34"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Point TensorBoard to the logs directory created by mlagents-learn\n",
        "%tensorboard --logdir ./results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKt2vsYoK56o"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew59mK19zjtN"
      },
      "source": [
        "Si vous ne souhaitez pas utiliser Google Colab ou Jupyter Notebook, vous devez utiliser √† la place la commande suivante : `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi0y_VASRzJU"
      },
      "source": [
        "Ensuite, il nous suffit d'ex√©cuter `mlagents-push-to-hf`.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/mlpush.png\" alt=\"ml learn function\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w0GUG__WMNtC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4fPfnczunT"
      },
      "source": [
        "Nous d√©finissons quatre param√®tres :\n",
        "1. `--run-id` : l‚Äôidentifiant de l‚Äôex√©cution d‚Äôentra√Ænement.\n",
        "2. `--local-dir` : l‚Äôemplacement o√π l‚Äôagent a √©t√© enregistr√©. Il s‚Äôagit de results/<nom_de_l‚Äôex√©cution>, soit results/First Training dans mon cas.\n",
        "3. `--repo-id` : le nom du d√©p√¥t Hugging Face que vous souhaitez cr√©er ou mettre √† jour. Il est toujours compos√© de <votre nom d‚Äôutilisateur Hugging Face>/<nom du d√©p√¥t>. Si le d√©p√¥t n‚Äôexiste pas, **il sera cr√©√© automatiquement**.\n",
        "4. `--commit-message` : les d√©p√¥ts Hugging Face √©tant des d√©p√¥ts Git, vous devez d√©finir un message de commit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGEFAIboLVc6"
      },
      "outputs": [],
      "source": [
        "!mlagents-push-to-hf --run-id=\"HuggyTraining\" --local-dir=\"./results/Huggy2\" --repo-id=\"ThomasSimonini/ppo-Huggy\" --commit-message=\"Huggy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yborB0850FTM"
      },
      "source": [
        "Sinon, si tout s'est bien pass√©, vous devriez obtenir ceci √† la fin du processus (mais avec une URL diff√©rente üòÜ) :\n",
        "\n",
        "``` Votre mod√®le a √©t√© envoy√© au hub. Vous pouvez le consulter ici :\n",
        " https://huggingface.co/ThomasSimonini/ppo-Huggy\n",
        "```\n",
        "\n",
        "Il s'agit du lien vers votre d√©p√¥t de mod√®le. Ce d√©p√¥t contient une fiche descriptive du mod√®le expliquant son utilisation, vos journaux TensorBoard et votre fichier de configuration. **L'avantage, c'est qu'il s'agit d'un d√©p√¥t Git : vous pouvez donc y ajouter des modifications, le mettre √† jour avec un nouveau push, ouvrir des pull requests, etc.**\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/modelcard.png\" alt=\"ml learn function\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uaon2cg0NrL"
      },
      "source": [
        "Mais voici le meilleur : **pouvoir jouer avec Huggy en ligne üëÄ.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMc4oOsE0QiZ"
      },
      "source": [
        "## Joue avec ton Huggy üêï\n",
        "Cette √©tape est tr√®s simple :\n",
        "- Ouvre le jeu Huggy dans ton navigateur : https://huggingface.co/spaces/ThomasSimonini/Huggy\n",
        "- Clique sur ¬´ Jouer avec mon mod√®le Huggy ¬ª\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/load-huggy.jpg\" alt=\"load-huggy\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djs8c5rR0Z8a"
      },
      "source": [
        "1. √Ä l'√©tape 1, saisissez votre nom d'utilisateur (attention √† la casse : par exemple, mon nom d'utilisateur est ThomasSimonini et non thomassimonini ou ThOmasImoNInI) et cliquez sur le bouton de recherche.\n",
        "2. √Ä l'√©tape 2, s√©lectionnez votre d√©p√¥t de mod√®les.\n",
        "3. √Ä l'√©tape 3, **choisissez le mod√®le que vous souhaitez rejouer** :\n",
        "- J'en ai plusieurs, car nous avons enregistr√© un mod√®le toutes les 500 000 it√©rations.\n",
        "- Mais comme je souhaite utiliser le plus r√©cent, je choisis `Huggy.onnx`.\n",
        "\n",
        "üëâ L'int√©r√™t est de **tester avec diff√©rents mod√®les pour observer l'am√©lioration de l'agent**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6dPWmh064H"
      },
      "source": [
        "F√©licitations pour avoir termin√© cette unit√© bonus !\n",
        "\n",
        "Tu peux maintenant t'installer confortablement et t'amuser avec ton Huggy üê∂. Et n'oublie pas de partager le plaisir de jouer avec Huggy avec tes amis ü§ó.\n",
        "\n",
        "Si tu en parles sur les r√©seaux sociaux, n'oublie pas de nous identifier (@huggingface) et moi (@simoninithomas).\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-cover.jpeg\" alt=\"Huggy cover\" width=\"100%\">\n",
        "\n",
        "\n",
        "## Continuez d'apprendre, restez g√©niaux ü§ó"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20164e44"
      },
      "source": [
        "!ls -l ./results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf19a48a"
      },
      "source": [
        "!ls -l ./results/Huggy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99de7eec"
      },
      "source": [
        "# Task\n",
        "I will modify cell `bS-Yh1UdHfzy` to use the absolute path `/content/ml-agents/config/ppo/Huggy.yaml` for Huggy.yaml in the mlagents-learn command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16534a22"
      },
      "source": [
        "## Modify Training Command\n",
        "\n",
        "### Subtask:\n",
        "Modify cell bS-Yh1UdHfzy to use the absolute path for Huggy.yaml in the mlagents-learn command.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f63c24c7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "* The `mlagents-learn` command in cell `bS-Yh1UdHfzy` is identified as needing modification.\n",
        "* The specific change required is to use the absolute path `/content/ml-agents/config/ppo/Huggy.yaml` for the `Huggy.yaml` configuration file within the command.\n",
        "\n",
        "### Insights or Next Steps\n",
        "* The immediate next step is to implement the planned modification in cell `bS-Yh1UdHfzy` to ensure the `mlagents-learn` command correctly references the configuration file.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}