{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metapat/Apprentissage-par-Renforcement-Profond/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3NL_e4crQv"
      },
      "source": [
        "# Bonus Unit√© 1 : Apprenons √† Huggy le chien üê∂ √† rapporter un b√¢ton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYrDriDujzX"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit2/thumbnail.png\" alt=\"Bonus Unit 1Thumbnail\">\n",
        "\n",
        "Dans ce cahier, nous consoliderons les acquis de la premi√®re unit√© en **apprenant √† Huggy le chien √† rapporter le b√¢ton, puis en jouant directement avec lui dans votre navigateur**.\n",
        "\n",
        "‚¨áÔ∏è Voici un exemple de ce que **vous serez capable de faire √† la fin de cette unit√©**. ‚¨áÔ∏è (cliquez sur ‚ñ∂ pour voir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnVhs1yYNyUF"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7oR6R-ZIbeS"
      },
      "source": [
        "### L'environnement üéÆ\n",
        "- Huggy le chien, un environnement cr√©√© par [Thomas Simonini](https://twitter.com/ThomasSimonini) based on [Puppo The Corgi](https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit)\n",
        "\n",
        "### La biblioth√®que utilis√©e üìö\n",
        "\n",
        "- [MLAgents](https://github.com/Unity-Technologies/ml-agents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60yACvZwO0Cy"
      },
      "source": [
        "Nous essayons constamment d'am√©liorer nos tutoriels, donc **si vous trouvez des probl√®mes dans ce notebook**, veuillez [ouvrir un probl√®me sur le d√©p√¥t Github](https://github.com/huggingface/deep-rl-class/issues).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oks-ETYdO2Dc"
      },
      "source": [
        "## Objectifs de ce cahier üèÜ\n",
        "\n",
        "√Ä la fin de ce cahier, vous serez capable de :\n",
        "- Comprendre **l‚Äôespace d‚Äô√©tats, l‚Äôespace d‚Äôactions et la fonction de r√©compense utilis√©s pour entra√Æner Huggy**.\n",
        "- **Entra√Æner votre propre Huggy** √† rapporter le b√¢ton.\n",
        "- Jouer **directement avec votre Huggy entra√Æn√© dans votre navigateur**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUlVrqnBv2o1"
      },
      "source": [
        "## Ce carnet provient du cours d'apprentissage par renforcement profond.\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAMjaQpHwB_s"
      },
      "source": [
        "Dans ce cours gratuit, vous allez :\n",
        "- üìñ √âtudier l‚Äôapprentissage par renforcement profond, tant sur le plan th√©orique que pratique.\n",
        "- üßë‚Äçüíª Apprendre √† utiliser des biblioth√®ques d‚Äôapprentissage par renforcement profond reconnues, telles que Stable Baselines3, RL Baselines3 Zoo, CleanRL et Sample Factory 2.0.\n",
        "- ü§ñ Entra√Æner des agents dans des environnements uniques.\n",
        "\n",
        "Pour en savoir plus, consultez le programme. üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "Le meilleur moyen de rester en contact est de rejoindre notre serveur Discord pour √©changer avec la communaut√© et avec nous üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r7Hl0uywFSO"
      },
      "source": [
        "## Pr√©requis üèóÔ∏è\n",
        "Avant de commencer, vous devez :   \n",
        "üî≤ üìö **Comprendre les fondements de l‚Äôapprentissage par renforcement** (module critique, d√©pendance temporelle, hypoth√®se des r√©compenses‚Ä¶) en r√©alisant l‚Äôunit√© 1.   \n",
        "üî≤ üìö **Lire l‚Äôintroduction de Huggy** en r√©alisant l‚Äôunit√© bonus 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DssdIjk_8vZE"
      },
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfCXHy68xBv"
      },
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an3ByrXYQ4iK"
      },
      "source": [
        "## Cloner le d√©p√¥t üîΩ\n",
        "- Nous devons cloner le d√©p√¥t contenant les **agents ML**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WNoL04M7rTa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Clone the repository (can take 3min)\n",
        "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kohJPf8u70aK"
      },
      "source": [
        "## Configuration de l'environnement virtuel üîΩ\n",
        "- Pour que **ML-Agents** fonctionne correctement dans Colab, la version de Python utilis√©e par Colab doit respecter les exigences de la biblioth√®que.\n",
        "- Vous pouvez v√©rifier la version de Python requise gr√¢ce au param√®tre `python_requires` dans les fichiers `setup.py`. Ces fichiers sont n√©cessaires √† la configuration de la biblioth√®que **ML-Agents** et se trouvent aux emplacements suivants :\n",
        "  - `/content/ml-agents/ml-agents/setup.py`\n",
        "  - `/content/ml-agents/ml-agents-envs/setup.py`\n",
        "- La version Python actuelle de Colab (v√©rifiable avec `!python --version`) ne correspond pas au param√®tre `python_requires` de la biblioth√®que. Par cons√©quent, l'installation peut √©chouer silencieusement et entra√Æner des erreurs comme celles-ci lors de l'ex√©cution ult√©rieure des m√™mes commandes :\n",
        "  - `/bin/bash: line 1: mlagents-learn: command not found`\n",
        "  - `/bin/bash: line 1: mlagents-push-to-hf: command not found`\n",
        "\n",
        "- Pour r√©soudre ce probl√®me, nous allons cr√©er un environnement virtuel avec une version de Python compatible avec la biblioth√®que **ML-Agents**.\n",
        "\n",
        "Remarque : *Pour assurer la compatibilit√© future, v√©rifiez toujours le param√®tre `python_requires` dans les fichiers d'installation et configurez votre environnement virtuel avec la version maximale de Python prise en charge dans le script ci-dessous si la version de Python de Colab n'est pas compatible.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikk36JNn70aK"
      },
      "outputs": [],
      "source": [
        "# Colab's Current Python Version (Incompatible with ML-Agents)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWJtlkyR70aK"
      },
      "outputs": [],
      "source": [
        "# Install virtualenv and create a virtual environment\n",
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "\n",
        "# Download and install Miniconda\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# Activate Miniconda and install Python ver 3.10.12\n",
        "!source /usr/local/bin/activate\n",
        "!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n",
        "\n",
        "# Set environment variables for Python and conda paths\n",
        "!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n",
        "!export CONDA_PREFIX=/usr/local/envs/myenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhE7brD770aK"
      },
      "outputs": [],
      "source": [
        "# Python Version in New Virtual Environment (Compatible with ML-Agents)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r60nFOdm70aK"
      },
      "source": [
        "## Installation des d√©pendances üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8wmVcMk7xKo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Go inside the repository and install the package (can take 3min)\n",
        "%cd ml-agents\n",
        "!pip3 install -e ./ml-agents-envs\n",
        "!pip3 install -e ./ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRY5ufKUKfhI"
      },
      "source": [
        "### T√©l√©charger et d√©placer le fichier zip de l'environnement dans\n",
        "`./trained-envs-executables/linux/`\n",
        "\n",
        "- Notre fichier ex√©cutable d'environnement se trouve dans une archive zip.\n",
        "- Il faut le t√©l√©charger et le placer dans le r√©pertoire appropri√©.\n",
        " `./trained-envs-executables/linux/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Ls6_6eOKiA"
      },
      "outputs": [],
      "source": [
        "!mkdir ./trained-envs-executables\n",
        "!mkdir ./trained-envs-executables/linux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHh_LXsRrrbM"
      },
      "source": [
        "Nous avons t√©l√©charg√© le fichier Huggy.zip depuis https://github.com/huggingface/Huggy en utilisant `wget`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xNAD1tRpy0_"
      },
      "outputs": [],
      "source": [
        "!wget \"https://github.com/huggingface/Huggy/raw/main/Huggy.zip\" -O ./trained-envs-executables/linux/Huggy.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FPx0an9IAwO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./trained-envs-executables/linux/ ./trained-envs-executables/linux/Huggy.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyumV5XfPKzu"
      },
      "source": [
        "Assurez-vous que votre fichier est accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdFsLJ11JvQf"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./trained-envs-executables/linux/Huggy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYKVj8yUvj55"
      },
      "source": [
        "## R√©capitulons le fonctionnement de cet environnement\n",
        "### L'espace d'√©tats : ce que Huggy ¬´ per√ßoit ¬ª. Huggy ne ¬´ voit ¬ª pas son environnement. Nous lui fournissons plut√¥t des informations √† son sujet :\n",
        "- La position de la cible (le b√¢ton)\n",
        "- Sa position relative par rapport √† la cible\n",
        "- L'orientation de ses jambes.\n",
        "\n",
        "Gr√¢ce √† ces informations, Huggy **peut d√©cider de la prochaine action √† entreprendre pour atteindre son objectif**.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.jpg\" alt=\"Huggy\" width=\"100%\">\n",
        "\n",
        "### L'espace d'action : les mouvements que Huggy peut faire\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-action.jpg\" alt=\"Huggy action\" width=\"100%\">\n",
        "\n",
        "Les jambes de Huggy sont actionn√©es par des moteurs articulaires. Cela signifie que pour atteindre la cible, Huggy doit apprendre √† faire pivoter correctement les moteurs articulaires de chacune de ses jambes afin de pouvoir se d√©placer.\n",
        "\n",
        "### La fonction de r√©compense\n",
        "\n",
        "La fonction de r√©compense est con√ßue pour que **Huggy atteigne son objectif** : rapporter le b√¢ton.\n",
        "\n",
        "Rappelons que l‚Äôun des fondements de l‚Äôapprentissage par renforcement est l‚Äô*hypoth√®se de r√©compense* : un objectif peut √™tre d√©crit comme la **maximisation de la r√©compense cumul√©e attendue**.\n",
        "\n",
        "Ici, notre objectif est que Huggy **se dirige vers le b√¢ton sans trop tourner sur lui-m√™me**. Par cons√©quent, notre fonction de r√©compense doit traduire cet objectif. Notre fonction de r√©compense :\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/reward.jpg\" alt=\"Huggy reward function\" width=\"100%\">\n",
        "\n",
        "- *Bonus d'orientation* : nous le **r√©compensons lorsqu'il s'approche de la cible.**\n",
        "- *P√©nalit√© de temps* : une p√©nalit√© de temps fixe est appliqu√©e √† chaque action afin de **l'obliger √† atteindre le b√¢ton le plus rapidement possible**.\n",
        "- *P√©nalit√© de rotation* : nous p√©nalisons Huggy **s'il tourne trop vite sur lui-m√™me.**\n",
        "- *R√©compense pour avoir atteint la cible* : nous r√©compensons Huggy **lorsqu'il atteint la cible.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuEq32Mwvtz"
      },
      "source": [
        "## Cr√©ez le fichier de configuration Huggy.\n",
        "- Dans ML-Agents, vous d√©finissez les **hyperparam√®tres d'entra√Ænement** dans des fichiers config.yaml.\n",
        "- Dans le cadre de ce notebook, nous n'allons pas modifier les hyperparam√®tres. Toutefois, si vous souhaitez faire des essais, vous pouvez modifier d'autres hyperparam√®tres. Unity fournit une documentation tr√®s compl√®te expliquant chacun d'eux [disponible ici](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
        "\n",
        "- Il nous faut cr√©er un fichier de configuration pour Huggy.\n",
        "  - Pour ce faire, cliquez sur l'ic√¥ne du dossier √† gauche de votre √©cran.\n",
        "\n",
        "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/create_file.png\" alt=\"Create file\" width=\"10%\">\n",
        "\n",
        "  - Allez √† `/content/ml-agents/config/ppo`\n",
        "  - Faites un clic droit et cr√©ez un nouveau fichier appel√© `Huggy.yaml`\n",
        "\n",
        "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/create-huggy.png\" alt=\"Create huggy.yaml\" width=\"20%\">\n",
        "\n",
        "- Copiez et collez le contenu ci-dessous üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loQ0N5jhXW71"
      },
      "outputs": [],
      "source": [
        "behaviors:\n",
        "  Huggy:\n",
        "    trainer_type: ppo\n",
        "    hyperparameters:\n",
        "      batch_size: 2048\n",
        "      buffer_size: 20480\n",
        "      learning_rate: 0.0003\n",
        "      beta: 0.005\n",
        "      epsilon: 0.2\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "      learning_rate_schedule: linear\n",
        "    network_settings:\n",
        "      normalize: true\n",
        "      hidden_units: 512\n",
        "      num_layers: 3\n",
        "      vis_encode_type: simple\n",
        "    reward_signals:\n",
        "      extrinsic:\n",
        "        gamma: 0.995\n",
        "        strength: 1.0\n",
        "    checkpoint_interval: 200000\n",
        "    keep_checkpoints: 15\n",
        "    max_steps: 2e6\n",
        "    time_horizon: 1000\n",
        "    summary_freq: 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oakN7UHwXdCX"
      },
      "source": [
        "- Don't forget to save the file!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9wv5NYGw-05"
      },
      "source": [
        "- **In the case you want to modify the hyperparameters**, in Google Colab notebook, you can click here to open the config.yaml: `/content/ml-agents/config/ppo/Huggy.yaml`\n",
        "\n",
        "- For instance **if you want to save more models during the training** (for now, we save every 200,000 training timesteps). You need to modify:\n",
        "  - `checkpoint_interval`: The number of training timesteps collected between each checkpoint.\n",
        "  - `keep_checkpoints`: The maximum number of model checkpoints to keep.\n",
        "\n",
        "=> Just keep in mind that **decreasing the `checkpoint_interval` means more models to upload to the Hub and so a longer uploading time**\n",
        "We‚Äôre now ready to train our agent üî•."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fI555bO12v"
      },
      "source": [
        "## Train our agent\n",
        "\n",
        "To train our agent, we just need to **launch mlagents-learn and select the executable containing the environment.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/mllearn.png\" alt=\"ml learn function\" width=\"100%\">\n",
        "\n",
        "With ML Agents, we run a training script. We define four parameters:\n",
        "\n",
        "1. `mlagents-learn <config>`: the path where the hyperparameter config file is.\n",
        "2. `--env`: where the environment executable is.\n",
        "3. `--run-id`: the name you want to give to your training run id.\n",
        "4. `--no-graphics`: to not launch the visualization during the training.\n",
        "\n",
        "Train the model and use the `--resume` flag to continue training in case of interruption.\n",
        "\n",
        "> It will fail first time when you use `--resume`, try running the block again to bypass the error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN32oWF8zPjs"
      },
      "source": [
        "The training will take 30 to 45min depending on your machine (don't forget to **set up a GPU**), go take a ‚òïÔ∏èyou deserve it ü§ó."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS-Yh1UdHfzy"
      },
      "outputs": [],
      "source": [
        "!mlagents-learn ./config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=\"Huggy2\" --no-graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vue94AzPy1t"
      },
      "source": [
        "## Push the agent to the ü§ó Hub\n",
        "\n",
        "- Now that we trained our agent, we‚Äôre **ready to push it to the Hub to be able to play with Huggy on your browserüî•.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izT6FpgNzZ6R"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n",
        "\n",
        "- Copy the token\n",
        "- Run the cell below and paste the token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKt2vsYoK56o"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew59mK19zjtN"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi0y_VASRzJU"
      },
      "source": [
        "Then, we simply need to run `mlagents-push-to-hf`.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/mlpush.png\" alt=\"ml learn function\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4fPfnczunT"
      },
      "source": [
        "And we define 4 parameters:\n",
        "\n",
        "1. `--run-id`: the name of the training run id.\n",
        "2. `--local-dir`: where the agent was saved, it‚Äôs results/<run_id name>, so in my case results/First Training.\n",
        "3. `--repo-id`: the name of the Hugging Face repo you want to create or update. It‚Äôs always <your huggingface username>/<the repo name>\n",
        "If the repo does not exist **it will be created automatically**\n",
        "4. `--commit-message`: since HF repos are git repository you need to define a commit message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGEFAIboLVc6"
      },
      "outputs": [],
      "source": [
        "!mlagents-push-to-hf --run-id=\"HuggyTraining\" --local-dir=\"./results/Huggy2\" --repo-id=\"ThomasSimonini/ppo-Huggy\" --commit-message=\"Huggy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yborB0850FTM"
      },
      "source": [
        "Else, if everything worked you should have this at the end of the process(but with a different url üòÜ) :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Your model is pushed to the hub. You can view your model here: https://huggingface.co/ThomasSimonini/ppo-Huggy\n",
        "```\n",
        "\n",
        "It‚Äôs the link to your model repository. The repository contains a model card that explains how to use the model, your Tensorboard logs and your config file. **What‚Äôs awesome is that it‚Äôs a git repository, which means you can have different commits, update your repository with a new push, open Pull Requests, etc.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/modelcard.png\" alt=\"ml learn function\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uaon2cg0NrL"
      },
      "source": [
        "But now comes the best: **being able to play with Huggy online üëÄ.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMc4oOsE0QiZ"
      },
      "source": [
        "## Play with your Huggy üêï\n",
        "\n",
        "This step is the simplest:\n",
        "\n",
        "- Open the game Huggy in your browser: https://huggingface.co/spaces/ThomasSimonini/Huggy\n",
        "\n",
        "- Click on Play with my Huggy model\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/load-huggy.jpg\" alt=\"load-huggy\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djs8c5rR0Z8a"
      },
      "source": [
        "1. In step 1, type your username (your username is case sensitive: for instance, my username is ThomasSimonini not thomassimonini or ThOmasImoNInI) and click on the search button.\n",
        "\n",
        "2. In step 2, select your model repository.\n",
        "\n",
        "3. In step 3, **choose which model you want to replay**:\n",
        "  - I have multiple ones, since we saved a model every 500000 timesteps.\n",
        "  - But since I want the more recent, I choose `Huggy.onnx`\n",
        "\n",
        "üëâ What‚Äôs nice **is to try with different models steps to see the improvement of the agent.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6dPWmh064H"
      },
      "source": [
        "Congrats on finishing this bonus unit!\n",
        "\n",
        "You can now sit and enjoy playing with your Huggy üê∂. And don't **forget to spread the love by sharing Huggy with your friends ü§ó**. And if you share about it on social media, **please tag us @huggingface and me @simoninithomas**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-cover.jpeg\" alt=\"Huggy cover\" width=\"100%\">\n",
        "\n",
        "\n",
        "## Keep Learning, Stay  awesome ü§ó"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}